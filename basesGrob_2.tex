\documentclass[a4page,10pt]{article}
\usepackage[Latin1]{inputenc}
\usepackage[francais]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{textcomp}
\usepackage{mathrsfs}
\usepackage{ algorithm , algorithmic }
\usepackage[all]{xy}
\usepackage{hyperref}

%%% francisation des algorithmes

\makeatletter

\renewcommand*{\ALG@name}{Algorithme}

\makeatother

\renewcommand{\algorithmicrequire}{\textbf{\textsc {Entrées  :  } } }
\renewcommand{\algorithmicensure}{\textbf{\textsc { Sortie  :  } } }
\renewcommand{\algorithmicwhile}{\textbf{Tant que}}
\renewcommand{\algorithmicdo}{\textbf{faire }}
\renewcommand{\algorithmicif}{\textbf{Si}}
\renewcommand{\algorithmicelse}{\textbf{Sinon}}
\renewcommand{\algorithmicthen}{\textbf{alors }}
\renewcommand{\algorithmicend}{\textbf{fin}}
\renewcommand{\algorithmicfor}{\textbf{Pour}}
\renewcommand{\algorithmicuntil}{\textbf{Jusqu'à}}
\renewcommand{\algorithmicrepeat}{\textbf{Répéter}}

\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\GL}{\mathcal{G}\mathcal{L}}
\newcommand{\SL}{\mathcal{S}\mathcal{L}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Rscr}{\mathscr{R}}
\newcommand{\Ebar}{\overline{E}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\fbar}{\overline{f}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}

\DeclareMathOperator{\multideg}{\mathrm{multideg}}
\DeclareMathOperator{\LM}{\mathrm{LM}}
\DeclareMathOperator{\LT}{\mathrm{LT}}
\DeclareMathOperator{\LC}{\mathrm{LC}}
\DeclareMathOperator{\PPCM}{\mathrm{PPCM}}
\DeclareMathOperator{\Syl}{\mathrm{Syl}}
\DeclareMathOperator{\Res}{\mathrm{Res}}
\DeclareMathOperator{\Com}{\mathrm{Com}}
\headheight=0mm
\topmargin=-10mm
\oddsidemargin=-1cm
\evensidemargin=-1cm
\textwidth=18cm
\textheight=22cm
\parindent=0mm
\begin{document}
\title{Bases de Gröbner}
\author{Antoine BOIVIN}
\maketitle


\newtheorem{Thm}{Théorème}[section]
\newtheorem{prop}[Thm]{Propriété}
\newtheorem{Lemme}[Thm]{Lemme}
\newtheorem{cor}[Thm]{Corollaire}

\theoremstyle{definition}

\newtheorem{Ex}[Thm]{Exemple}
\newtheorem{Def}[Thm]{Définition}
\newtheorem{Not}[Thm]{Notation}
\theoremstyle{remark}
\newtheorem{Rq}[Thm]{Remarque}


\input{intro_BG} 
\section{Résolution de système polynomiaux}
Dans cette section, nous allons étudier comment utiliser les bases de Gröbner afin de résoudre les systèmes d'équations polynomiales. On va commencer par donner l'exemple des polynômes de degré $1$ qui peuvent se résoudre avec le pivot de Gauss et voir que les idées du pivot peuvent se généraliser pour toutes familles de polynômes.
\subsection{Exemples préliminaires}
Comme dit plus haut, lorsque les $f_i$ sont des polynômes de degré 1, on peut résoudre les systèmes de la forme $$f_i(x_1,\ldots,x_n)=0, 1 \leq i \leq s \; (*)$$ en utilisant le pivot de Gauss.\\ 
Le système $(*)$ se résume à : $AX=B$ où $A$ est la matrice représentative de la partie linéaire de $(f_1,\ldots,f_s)$ et $-B$ sa partie constante.\\
Avec le pivot de Gauss, on obtient une matrice échelonnée $\widetilde{A}$ et une colonne de constante $\widetilde{B}=\begin{pmatrix} \widetilde{b_1} \\ \vdots \\ \widetilde{b_s} \end{pmatrix}$ tel que : \\$(*) \Leftrightarrow \widetilde{A}X=\widetilde{B}$. \\
Ce qui nous permet de trouver les solutions de $(*)$ : \\
Soit $r=rg(A)(\leq \min(n,s))$
\begin{itemize}
	\item S'il existe un $\widetilde{b_k}$ avec $k>r$ alors $(*)$ n'a pas de solution. \\
\item Si $r<n$ alors on peut exprimer les $r$ premières coordonnées des solutions de $(*)$ grâce aux $n-r$ dernières. L'ensemble des solutions de $(*)$ est un espace affine de dimension $n-r$.
\item Si $r=n$ alors $(*)$ admet une seule solution.
\end{itemize}
~\\
On peut voir que le pivot de Gauss consiste à obtenir, à partir des polynômes données au départ, des polynômes ayant une variable en moins puis continuer jusqu'à ce que l'on obtienne un système échelonné. On "remonte" ensuite le système afin de trouver les solutions du système. \\
Dans la suite, nous allons montrer le théorème d'élimination qui va nous permettre d'éliminer les variables dans les système polynomiaux et ensuite le théorème d'extension pour obtenir les solutions dudit système à partir de celles obtenues par les sytèmes ayant des variables éliminées .

\subsection{Théorème d'élimination}
\begin{Def}
Soit $I=\left\langle f_1,\ldots,f_s \right\rangle $ un idéal de $k[X_1,\ldots,X_n]$. On appelle $p$ème idéal d'élimination de $I$ l'idéal $I_p$ de $k[X_{p+1},\ldots,X_n]$ définit par : $I_p=k[X_{p+1},\ldots,X_n] \cap I$, $0 \leq p \leq n-1$
\end{Def}
\begin{Thm}[d'élimination]
Soit $I $ un idéal de $k[X_1,\ldots,X_n]$ et $G$ une base de Gröbner de $I$ selon l'ordre lexicographique (que l'on notera ici seulement $\geq$). Alors, pour tout $p \in [\![0,n]\!]$, l'ensemble $G_p =G \cap k[X_{p+1},\ldots,X_n]  $ est une base de Gröbner du $p$ème idéal d'élimination $I_p$.
\end{Thm}

\begin{proof}

Soit $p \in [\![0,n]\!]$. Posons $G=\{g_1,\ldots,g_m\}$ et tel que $G_p=\{g_1,\ldots,g_r\}$ (quitte à renommer les éléments). \\
Montrons que $G_p$ est une base de $I_p$. \\
Comme $G_p \subset I_p$ (car $G \subset I$) alors $\left \langle G_p \right\rangle \subset I_p$.\\
On peut écrire, grâce à l'algorithme de division, $f \in I$ sous la forme $f=\sum_{k=1}^m h_ig_i$, l'absence de reste est assuré par le fait que $G$ est une base de Gröbner de $I$ et $f \in I$.\\
Or pour tout $p>r,  \LM(g_i) > X^{p+1} \geq \LM(f) $ et donc aucun terme de $f$ ne peut être divisible par un $\LT(g_i)$. L'algorithme n'incrémente pas les $h_p$(avec $p >r$) et donc ceux-ci sont tous nuls.  \\
D'où, $f=\sum_{k=1}^r h_ig_i$ et donc $f_p \in \left \langle G_p \right\rangle$, ce qui finit de montrer l'égalité $\left \langle G_p \right\rangle = I_p$.\\
(Le même argument permet de montrer que si $f \in I_p$, $\overline{f}^G=\overline{f}^{G_p}$ $(*)$ ).\\
Montrons maintenant que $G$ est une base de Gröbner de $I_p$.\\
Il suffit, pour cela, de montrer que pour tout $1 \leq i < j \leq r$, $\overline{S(g_i,g_j)}^{G_p}=0$.\\
Soit $i,j \in [\![1,r]\!]$,$ i<j$.\\
Comme $S(g_i,g_j)$ est de la forme $Pg_i+Qg_j$ ($P,Q \in k[X_{p+1},\ldots,X_n]$) et $I_p$ est un idéal alors $S(g_i,g_j) \in I_p \subset I$ d'où comme $G$ est une base de Gröbner alors $\overline{S(g_i,g_j)}^{G}=0$ et donc d'après $(*)$, $\overline{S(g_i,g_j)}^{G_k}=0$. Ce qui permet de conclure.
\end{proof}
\begin{Ex}
Avec les $f,g,h$ de l'exemple \ref{ex}, le premier idéal d'élimination est : \\$I_1=\left\langle X - YZ^2 + Y, Y^2 + 2*Z^2 - 5, Z^4 - 7/2Z^2 + 3\right\rangle$ et le second $I_2=\left\langle X - YZ^2 + Y, Y^2 + 2Z^2 - 5, Z^4 - 7/2Z^2 + 3\right\rangle$
\end{Ex}
Lorsque l'on a calculé le $(n-1)$ème idéal d'élimination, on obtient lorsqu'il n'est pas réduit à 0, un idéal de la forme $\left\langle P\right\rangle$ (car $k[X_n]$ est principal) et donc un nombre fini de solutions. On va essayer d'étendre les solutions partielles des idéaux d'élimination en des solutions de l'idéal de départ. Pour cela, on va commencer par présenter des résultats sur les résultants qui vont permettre de montrer le théorème d'extension.
\subsection{Résultant}
Soient $A $ un anneau commutatif intègre et $K $ son corps de fractions.\\
Soient $f\in A[X] $ et $g \in A[X] $. Soient $p,q \in \N$, tels que $p\geq\deg f $ et $q\geq\deg g $.\\
Soit $\varphi_{f,g} : K_{q}[X]\times K_{p}[X] \to K_{p+q}[X] $ l'application linéaire définie par : $\forall (u,v) \in K_{q}[X]\times K_{p}[X], \varphi(u,v)=fu+gv $.\\
On appelle matrice de Sylvester la matrice associée à l'application $\varphi_{f,g} $ dans la base $\B=((X^i,X^j) | 0 \leq i \leq q-1 ; 0 \leq j \leq p-1) $ que l'on notera $\Syl_{p,q}(f,g)$.\\
On notera $\Res_{p,q}(f,g)$ le déterminant de $\Syl_{p,q}(f,g)$
On appelle résultant de $f $ et $g $  l'élément $\Res_{\deg(f),\deg(q)}(f,g)$.

\begin{Not}
\begin{itemize}
	\item On note $Z(I)$ l'ensemble des zéros communs à tous les polynômes de $I$ i.e. //$Z(I):=\{a \in k^n | \forall f \in I, f(a)=0 \}$.
	\item Soient $f,g \in k[X_1,\ldots,X_n]$. On note par $\Syl_{X_i}(f,g), \Res_{X_i}(f,g)$ la matrice de Sylvester (resp. résultant) de $f$ et $g$ vus comme polynômes de $k[X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n][X_i]$
\end{itemize}

\end{Not}
\begin{prop}
 $\Res (f, g)\in\left\langle f,g \right \rangle \cap A $

\end{prop}

\begin{proof}

 $\Res (f,g) \in A $ car le déterminant est polynomial en les coordonnées de la matrice et les composantes de $\Syl(f,g) $ sont dans $A$. \\
Montrons maintenant que $\Res (f,g) \in \left\langle f,g \right\rangle $.\\
Notons $M $ la matrice de Sylvester de $f $ et $g $. \\
Alors $M {}^t \Com(M)=\det (M)I_{\deg(f)+\deg(g)} $ $(*) $ où $\Com(M)$ est la matrice des cofacteurs de $M$. \\
Comme les cofacteurs de $M $ sont dans $A $ (car ceux-ci sont polynomiaux en les coordonnées) alors $\Com (M) $ est dans $\Mcal_{\deg(f)+\deg(g)}(A) $. \\
Donc, en posant $M=(C_1,\ldots,C_{\deg(f)+\deg(g)})$, on a, d'après $(*) $, : \\
$MC_1=\begin{pmatrix} \det (A) \\ 0 \\ \vdots\\ 0 \end{pmatrix} $. \\
En notant par $c $ l'élément de $ A_{\deg(g)}[X]\times A_{\deg(f)}[X] $ de colonne de coordonnées $C_1$ dans la base $\B  $ définie plus haut, on a : $\varphi_{f,g}(c)=\det (A) $, ce qui permet de conclure.
\end{proof}
\begin{cor}
Si $A=k[X_2,\ldots,X_n]$ alors $Res(f,g) \in I_1$
\end{cor}
\begin{Rq}
\begin{enumerate}
\item Si $f=a_1(X_2,\ldots,X_n)X_1+a_0(X_2,\ldots,X_n)$ et $g=b_1(X_2,\ldots,X_n)X_1+b_0(X_2,\ldots,X_n)$ sont des polynômes de degré 1  alors leur résultant $a_0b_1-a_1b_0$, ce qui est le m\^eme résultat que l'on obtient lorsqu'on applique le pivot de Gauss à $f$ et $g$.
\item Lorsqu'on considère un idéal engendré par $n$ polyn\^omes premiers entre eux, on peut utiliser le résultant afin d'avoir une idée suffisante de $I_{n-1}$ pour trouver toutes les solutions. En effet, à l'instar du pivot de Gauss utilisé pour les polyn\^omes de degré 1, on peut utilisé le résultant pour éliminer les variables d'un système : 
Soit $I:=\left \langle f_1,\ldots,f_n \right\rangle$ alors les $n-1$ polynômes $\Res_{X_1}(f_1,f_2),\ldots, \Res_{X_1}(f_1,f_n)$ sont dans $I_1$. En itérant ce processus, on obtient un polyn\^ome $f$  non nul (car les $f_i$ sont premiers entre eux) qui est dans $I_{n-1}$. Comme $k[X_n]$ est principal alors $I_{n-1}$ est engendré par un seul polyn\^ome $P$  qui divise $f$. D'où l'ensemble des zéros de $P$ est inclus dans celui de $f$.
\end{enumerate}
\end{Rq}
\begin{Ex} En reprenant les polynômes de l'exemple \ref{ex}, on a $P:=\Res_{X}(f,g)=g^2$, $Q:=\Res_{X}(f,h)=Y^4+Y^2Z^2-4Y^2+1$. Ensuite on a $\Res_Y(P,Q)=4Z^8 - 28Z^6 + 73Z^4 - 84Z^2 + 36=4(Z^4-7/2Z^2+3)^2$
\end{Ex}


\begin{prop}
\begin{itemize}
\label{result_deg}
    \item Si $p=deg(A)$ et $q=deg(B)$ alors, par d\'efinition, $Res(A,B)=Res_{p,q}(A,B)$
    \item Si $p=deg(A)$ et $q>deg(B)$ alors $Res_{p,q}(A,B)=((-1)^p a_p)^{q-deg B}Res(A,B)$
    \item Si $p>deg(A)$ et $q=deg(B)$ alors $Res_{p,q}(A,B)=b_q^{p-deg A}Res(A,B)$
    \item Si $p>deg(A)$ et $q>deg(B)$ alors $Res_{p,q}(A,B)=0$
\end{itemize}
\end{prop}
\begin{Thm}
\label{zeros}
Supposons le corps $K$ alg\'ebriquement clos. Alors $Res(A,B)=0$ si, et seulement si, les polyn\^omes $A$ et $B$ ont une racine commune.
\end{Thm}
C'est ce théorème qui va faire marcher la preuve du théorème d'extension.
\subsection{Théorème d'extension}

\begin{Thm}[d'extension]

 Soit $I=\left \langle f_1,\ldots,f_s \right\rangle$ un idéal de $\C[X_1,\ldots,X_n]$ et $I_1$ le premier idéal d'élimination.\\
Ecrivons, pour $i \in [\![1,s]\!]$, $f_i$ sous la forme : 
$f_i=g(X_2,\ldots,X_n)X_1^{N_i}$+termes de degré $<N_i$ en $X_1$\\
où $N_i \geq 0$ et $g_i \in \mathbb{C}[X_2,\ldots,X_n]$ non nul si $f_i \neq 0$ ($g_i=0$ si $f_i=0$). \\
Supposons qu'on ait une solution partielle $(a_2,\ldots,a_n) \in Z(I_1)$. Si $(a_2,\ldots,a_n) \notin Z(g_1,\ldots,g_s)$ alors il existe $a_1 \in \mathbb{C}$ tel que $(a_1,\ldots,a_n) \in Z(I)$.
\end{Thm}
\begin{proof}
Montrons ce résultat pour $s=2$. \\
Soient $f=\sum_{i=0}^p f_i(X_2,\ldots,X_n)X_1^i$ et $g=\sum_{j=0}^q g_j(X_2,\ldots,X_n)X_1^j$ où $f_p \neq 0 \neq g_q$. \\
Soit $I$ l'idéal engendré par $f$ et $g$ et $I_1$ son premier idéal d'élimination.\\
Soit $c=(c_2,\ldots,c_n) \in Z(I_1)$.\\
Montrons que soit $f_p(c)=g_q(c)=0$ soit il existe $c_1 \in k$ tel que $(c_1,\ldots,c_n) \in Z(I)$.\\
Pour cela, montrons, tout d'abord, que $\Res_{X_1}(f,g)(c)=\Res_{p,q}(f(X_1,c),g(X_1,c))$.\\
Considérons,pour cela, le morphisme $\varphi : k[X_1,\ldots,X_n]\to k[X_1]$ défini par : 
$\forall f \in k[X_1,\ldots,X_n],\varphi(f)=f(X_1,c)$.\\
On a donc $\Res_{X_1}(f,g)(c)=\varphi(\Res_{X_1}(f,g))$ (en identifiant $k[X_2,\ldots,X_n]$ avec le sous-anneau de $k[X_1,\ldots,X_n]$ des polynômes de degré 0 en $X_1$).
Notons $\left(a_{i,j}(X_2,\ldots,X_n)\right)$ les coefficients de $\Syl_{X_1}(f,g)$.
Alors, comme $\varphi$ est un morphisme d'anneaux, $\Res_{X_1}(f,g)(c)=\varphi(\Res_{X_1}(f,g))=\varphi(\det\left(a_{i,j}(X_2,\ldots,X_n)\right))$. \\
De plus, comme $c \in Z(I_1)$ et que $Res_{X_1}(f,g) \in I_1$ alors $\Res_{X_1}(f,g)(c)=0$ et donc $\Res_{p,q}(f(X_1,c),g(X_1,c))=0$.\\
Supposons maintenant que $f_p(c) \neq 0$. \\
Alors, d'après les propriétés du résultant (c.f. lemme \ref{result_deg}), on a : $\Res(A,B)=\left((-1)^pf_p(c)\right)^{deg(f(X_1,c))-q}Res_{p,q}(A,B)=0$.
Ce qui permet de conclure que $f(X_1,c)$ et $g(X_1,c)$ ont une racine commune $c_1$(grâce au théorème \ref{zeros}) et donc $(c_1,\ldots,c_n) \in Z(I)$.
On obtient la m\^eme chose en supposant $g_q(c) \neq 0$.
\end{proof}
\begin{cor}

 Soit $I=\left \langle f_1,\ldots,f_s \right\rangle$ un idéal de $\mathbb{C}[X_1,\ldots,X_n]$ et supposons qu'il existe $i \in [\![1,n]\!]$ tel que $f_i$ s'écrit de la forme :
$f_i=cX_1^{N}$+termes de degré $<N$ en $X_1$\\
où $N> 0$ et $c \in \mathbb{C}\setminus \{0\}$. Si $I_1$ est le premier idéal d'élimination de $I$ et $(a_2,\ldots,a_n) \in Z(I_1)$ alors il existe $a_1 \in \mathbb{C}$ tel que $(a_1,\ldots,a_n) \in Z(I)$
\end{cor}
On va commencer par donner un contre-exemple montrant que toutes les hypothèses sont nécessaires.
\begin{Ex}
Soit $S_1$ le syst\`eme $\begin{cases}x^2=y \\ x^2=z \end{cases}$ et son ensemble de solutions $Z(X^2-Y,X^2-Z)$. Notons $I=\left\langle X²-Y,X²-Z \right\rangle$ et $I_1$ son premier idéal d'élimination.\\
On peut calculer une base de Gröbner de $I$ : $I=\left\langle X^2-Z,Y-Z\right\rangle$ d'où $I_1=<Y-Z>$. Et donc $Z(I_1)=\{(c,c) | c \in k\}$ \\
On peut remarquer que les termes dominants de $X^2-Y$ et $X^2-Z$ ne s'annulent pas. D'où, d'après le théorème d'extension, on peut étendre toutes les solutions partielles dans $\C$.\\
Si on travaille dans $\R$, on peut étendre la solution $(c,c)$ en une solution de $S_1$ si, et seulement si $c\geq 0$ car les solutions de $S_1$ ont leur deuxième et troisième coordonnées positives. \\
Soit $S_2$ le syst\`eme $\begin{cases}xy=1 \\ xz=1 \end{cases}$ et $I=\left\langle XY-1,XZ-1 \right\rangle$. \\
On peut calculer une base de Gröbner de $I$ : $I=\left\langle xz-1,y-z\right\rangle$ d'où $I_1=<y-z>$.
On en déduit que $Z(I_1)=\{(c,c) | c \in \C\}$ \\
On peut étendre toutes les solutions partielles sauf la solution $(0,0)$ (car $0x=1$ n'a pas de solutions) où les termes dominants de $XY-1$ et $XZ-1$ en $X$ s'annulent
\end{Ex}
On va maintenant finir d'étudier notre exemple (celui de \ref{ex}).
\begin{Ex}
On a $I_1=\left\langle g,Z^4-7/2Z^2+3 \right\rangle$ et $I_2=\left\langle Z^4-7/2Z^2+3 \right\rangle$. \\
On a $Z(I_2)=Z(\left\langle Z^4-7/2Z^2+3\right\rangle)=\{\pm \sqrt{2},\pm \sqrt{3/2}\}$. Puis, comme le terme dominant de $g$ est constant alors on peut utiliser le théorème d'extension pour montrer que $Z(I_1)=\{(\pm 1,\pm \sqrt{2}),(\pm\sqrt{2},\pm \sqrt{3/2})\}$ puis de la m\^eme façon, on a que $Z(I)=\{\pm(1,1,\pm\sqrt{2}),\pm(1/\sqrt{2},\sqrt{2},\pm\sqrt{2})\}$
\end{Ex}
Ces deux théorèmes permettent de répondre à la troisième question de l'introduction.
\subsection{Géométrie de l'élimination}
Nous allons, dans cette sous-section, donner une interprétation géométrique aux théorèmes d'élimination et d'extension. On va commencer par donner quelques résultats de géométrie.
\subsubsection{Résultats préliminaires}
\begin{Def}
Soit $f_1,\ldots,f_s$ des polynômes de $k[X_1,\ldots,X_n]$.\\ On appelle variété affine définie par $f_1,\ldots,f_s$ l'ensemble : $Z(f_1,\ldots,f_s)=\{(a_1,\ldots,a_n) \in k^n | \forall i \in [\![1,s]\!],f_i(a_1,\ldots,a_n)=0 \}$.
\end{Def}
\begin{Ex}
Dans $\R^2$, le cercle ($=Z(X^2+Y^2-R^2)$ où $R\in \R$), la parabole ($=Z(2pX^2-Y)$ où $p\in \R$) et plus généralement, les coniques sont des variétés affines. \\
Les sous-espaces affines de $k^n$ sont aussi des variétés affines.
\end{Ex}
\begin{Lemme} : Si $V,W \subset k^n$ sont des variétés affines alors $V \cap W$ aussi.
\end{Lemme}
\begin{proof}
Supposons que $V=Z(f_1,\ldots,f_s)$ et $W=Z(g_1,\ldots,g_r)$. 
Alors $V \cap W=Z(f_1,\ldots,f_s,g_1,\ldots,g_r)$.\\
En effet,$x \in V \cap W$ si, et seulement si, pour tous $i,j$, $f_i(x)=g_j(x)=0$, c'est-à-dire si, et seulement si, $x \in Z(f_1,\ldots,f_s,g_1,\ldots,g_r)$.
\end{proof}

\begin{prop} Si $\{f_1,\ldots,f_s\}$ et $\{g_1,\ldots,g_r\}$ sont des bases d'un même idéal $I$ de $k[X_1,\ldots,X_n]$ alors $Z(f_1,\ldots,f_s)=Z(g_1,\ldots,g_r)$
\end{prop}
\begin{proof}
Comme $\{f_1,\ldots,f_s\}$ et $\{g_1,\ldots,g_r\}$ sont des bases d'un même idéal alors on peut écrire les $g_i$ sous la forme $\sum_{i=1}^s h_{i,j}f_i$. De ce fait, si $x \in Z(f_1,\ldots,f_s)$ alors pour tout $j$, $f_j(x)=0$  et donc $g_i(x)=sum_{i=1}^s h_{i,j}(x)f_i(x)=0$. Et donc $x\in Z(g_1,\ldots,g_r)$.\\
Par symétrie de rôles, on obtient l'inclusion réciproque.
\end{proof}
En particulier, $Z(f_1,\ldots,f_s)=Z(\left\langle f_1,\ldots,f_s\right\rangle)$
\begin{Def} 
\label{Def_proj}
Soit $\pi_p $ la projection $\C^n \to \C^{n-p} $ définie par : $\forall (a_1,\ldots,a_n) \in \mathbb{C}^n, \pi_p (a_1,\ldots,a_n) =(a_{p+1},\ldots,a_n) $.
\end{Def}
\begin{prop}
Pour tout $1 \leq p \leq n-1$, la projection $\pi_p$ est une application surjective.
\end{prop}
\subsubsection{Géométrie de l'élimination}
Soit $V=Z(f_1,\ldots,f_s) \subset \C^n $
\begin{Lemme}
\label{lemme_inclus}
Soit $I_p $ le $p $ème idéal d'élimination de l'idéal $\left\langle f_1,\ldots,f_s \right \rangle $ de $\C[X_1,\ldots,X_n] $.\\ Alors, dans $\C^{n-p} $, $\pi_p (V) \subset Z (I_p) $.
\end{Lemme}
\begin{proof}
 Pour montrer cette égalité, il faut montrer que $\forall a \in \pi_p (V),\forall f \in I_p, f (a)=0$.\\
Soient $a=(a_{p+1},\ldots,a_n) \in \pi_p(V) $ et $f\in I_p  $.\\
Comme $\pi_p $ est surjective alors il existe un $a'=(a_1,\ldots,a_n) $ qui appartient à $V $. Alors $f (a') =0$ (car $f \in \left\langle f_1,\ldots,f_s \right\rangle$). Or comme $f $ ne dépend que de $X_{p+1},\ldots,X_n$ alors $f (a)=f (a')=0$. \\
\end{proof}
\begin{Thm}  Soit $g_i $ défini dans le théorème d'extension et $I_1$ le premier idéal d'élimination de $\left\langle f_1,\ldots,f_s \right\rangle$. On a alors l'égalité, dans $\C^{n-1} $,\\
$Z (I_1)=\pi(V) \cup (Z (g_1,\ldots,g_s) \cap Z (I_1))$
\end{Thm}
\begin{proof}
$\supset $ : c.f. Lemme \ref{lemme_inclus} \\
$\subset  $ : Soit $a:=(a_2,\ldots,a_n) \in Z (I_1)$ .
Alors si $a\notin\left\langle g_1,\ldots,g_s \right \rangle $, on a, d'après le théorème d'extension, l'existence d'un $a_1 \in \C $ tel que $(a_1,\ldots,a_n) \in V$ et donc  $a \in \pi_1 (V) $.\\
Sinon $a \in \left\langle g_1,\ldots,g_s \right \rangle$ et donc dans $\left\langle g_1,\ldots,g_s \right \rangle \cap Z (I_1) $
\end{proof}
En d'autres termes, d'après le théorème d'extension, une solution partielle de $Z(I_1)$ peut soit être étendue en une solution de $Z(I)$ (\emph{i.e.} appartient à $\pi(V)$) soit est un zéro commun des $g_i$. \\

Le théorème suivant décrit de manière plus précise le lien entre les projections de variété et les zéros de ses idéaux d'élimination.
\begin{Thm}[de fermeture]

 Soit $V=Z(f_1,\ldots,f_s) \subset \C^n$ et soit $I_p$ le $p$ème idéal d'élimination de $\left\langle f_1,\ldots,f_s \right\rangle$. Alors  
\begin{enumerate}
\item $Z(I_p)$ est la plus petite (au sens de l'inclusion) variété contenant $\pi_p(V)$

\item Si $V \neq \emptyset$, alors il existe une variété affine $W \subsetneqq Z(I_p)$ telle que $Z(I_p) \setminus W \subset \pi_p(V)$
\end{enumerate}
\end{Thm}
\begin{cor}
Supposons qu'il existe $i \in [\![1,n]\!]$ tel que $f_i$ s'écrit de la forme : \\
$f_i=cX_1^{N}$+termes de degré $<N$ en $X_1$ où $N> 0$ et $c \in \mathbb{C}\setminus \{0\}$.  \\Alors $\pi(V)=Z(I_1)$.
\end{cor}

\section{Implicitation}
Nous allons maintenant étudier le dernier problème que l'on s'est posé : l'implicitation.\\
Soit $S$ l'ensemble param\'etr\'e par le syst\`eme suivant : $ \begin{cases}
x_1=f_1(t_1,\ldots,t_m)& \\
\vdots & (\dagger) \\
x_n=f_n(t_1,\ldots,t_m)& \\
\end{cases} $\\ o\`u $f_i \in k[T_1,\ldots,T_m]$ et $(t_1,\ldots,t_m) \in k^m$ .\\
On peut voir $S$ comme l'image de la fonction $F : k^m \to k^n$ d\'efinie par : 
$\forall t \in k^m, F(t)=(f_1(t),\ldots,f_n(t))$.\\
$S$ n'est pas n\'ecessairement une vari\'et\'e affine (\emph{c.f.} exemple \ref{Whitney}).\\
Le syst\`eme $(\dagger)$ d\'efini tout de même une vari\'et\'e $V=Z(X_1-f_1,\ldots,X_n-f_n) \subset k^{n+m}$.\\
On a donc $V=\{(t_1,\ldots,t_n,x_1,\ldots,x_m)\in k^{n+m} | \forall i \in [\![1,m]\!],x_i-f_i(t_1,\ldots,t_n)=0 \}$\\
D'o\`u,  $V=\{(t_1,\ldots,t_n,f_1(t_1,\ldots,t_n),\ldots,f_m(t_1,\ldots,t_n))\in k^{n+m} | (t_1,\ldots,t_m) \in k^m \} $. Autrement dit, $V$ est le graphe de $F$.\\
De plus, on peut remarquer que $V$ est l'image de l'application 
$\begin{array}{ccccc}
i & : & k^m & \to & k ^{n+m} \\
 & & (t_1,\ldots,t_m) & \mapsto & (t_1,\ldots,t_n,f_1(t_1,\ldots,t_n),\ldots,f_m(t_1,\ldots,t_n))\\
\end{array}$\\
On en déduit le diagramme suivant :  \\
 \xymatrix{
   &k^{n+m} \ar[rd]^{\pi_m}&  \\
    k^m \ar[ru]^i\ar[rr]^F&& k^n
  }
i.e. $F=\pi_m \circ i$.\\
où $\pi_m : k^{n+m} \to k^n$ est une projection définie de manière analogue à \ref{Def_proj}. \\
On a donc $\pi_m(V)=F(k^m)$.\\
Autrement dit, l'image d'une param\'etrisation est la projection de son graphe.
\begin{Thm}
Soit $F:k^m \to k^n$ une fonction d\'etermin\'ee par la param\'etrisation polynomiale $(\dagger)$.\\ Soit $I$ l'id\'eal $\left\langle X_1-f_1,\ldots,X_n-f_n \right\rangle \subset k[T_1,\ldots,T_m,X_1,\ldots,X_n]$ et $I_m$ son $m$ \`eme id\'eal d'\'elimination. Alors $Z(I_m)$ est le plus petit id\'eal de $k^n$ contenant $F(k^m)$.
\end{Thm}
\begin{proof}
Supposons que $k$ est un sous-corps de $\C$. \\
Si $k=\C$ alors, d'après le théorème de fermeture, $Z(I_m)$ est la plus petite variété contenant $\pi_m(V)=F(k^m)$. \\
Si $k$ est un sous-corps strict de $\C$ alors on ne peut pas utiliser le théorème de fermeture immédiatement. \\
Posons $Z_k(I):=\{ x \in k^n | \forall f \in I, f(x)=0\} \subset Z_\C(I):=\{ x \in \C^n | \forall f \in I, f(x)=0\}$. \\
On cherche à montrer que $Z_k(I_m)$ est la plus petite variété de $k^n$ contenant $F(k^m)$. \\
Remarquons, tout d'abord, que $F(k^m)=\pi_m(V) \subset Z_k(I_m)$ (\emph{c.f.} lemme \ref{lemme_inclus}).\\ 
Ensuite, considérons une variété $V_k:=Z_k(g_1,\ldots,g_s)$ contenant $F(k^m)$ et montrons que $Z_k(I_m) \subset V_k$. \\
Soit $i \in [\![1,s]\!]$. Alors $g_i$ s'annule sur $V_k$(par définition) et en particulier sur $F(k^m)$. D'où $g_i\circ F$ s'annule sur $k^m$. \\ Comme $g_i \circ F \in k[T_1,\ldots,T_m]$ et que $k$ est infini, alors $g_i \circ F$ est le polynôme nul. \\
On en déduit que $g_i \circ F$ s'annule sur $\C^m$ ou encore que $g_i$ s'annule sur $F(\C^m)$. \\
D'où $F(\C^m) \subset V_\C$. D'après ce que l'on a montré pour le cas $k=\C$, on a $Z_\C(I_m) \subset V_\C$. En intersectant par $k^n$ à gauche et à droite de l'inclusion, on obtient $Z_k(I_m) \subset V_k$.
\end{proof}
\begin{Ex}
\label{Whitney}
On appelle parapluie de Withney l'ensemble $\Gamma_k$ paramétré par $\begin{cases} x=uv \\ y=u\\ z=v^2\end{cases} $ pour $u,v \in k^2$. On note par $F $  la fonction $(u,v)\mapsto (x,y,z) $. \\
En appliquant le théorème précédent, on obtient que $V_k:=Z( X^2-Y^2Z)\subset k^n $ est la plus petite variété contenant $\Gamma_k $. Examinons maintenant s'il y a une égalité :
\begin{itemize}
\item Supposons que $k=\R $ et considérons la variété $G_a:= \{z=a\}\cap V_\R$ pour $a\in \R $. Si $a <0$ alors l'équation $x^2-ay^2=0$ a pour unique solution $(0,0) $. De ce fait, $G_a=\{(0,0,a)\} $. Cependant ce point n'appartient pas à  $\Gamma_\R$ car $z\geq 0$.\\
Si $a \geq 0$ alors, on a l'équivalence  : $x^2-ay^2=0 \Leftrightarrow x=\pm \sqrt{a}y$ et donc $G_a=\{(\pm\sqrt{a}y,y,a)| y \in \R \}=\{F (y,\pm\sqrt{a}) | y \in \R \} \subset \Gamma $.
On en conclut que $V_\R=\Gamma_\R\cup \{ (0,0,a) | a <0 \} $. De ce fait, $\Gamma_\R$ n'est pas une variété.
\item Si $k=\C $, la situation est différente : \\
Soient  $a \in \C$, $\alpha $ tel que $\alpha ^2=a $ et $G_a:=\{z=a\} \cap V_\C $. \\
On a l'équivalence suivante : $x^2-ay^2=0 \Leftrightarrow x=\pm \alpha y $. De ce fait, $G_a=\{F (y,\pm \alpha)| y \in k\} \subset \Gamma_\C $. \\
Ce qui permet de conclure que $V_\C=\Gamma_\C $.\\
\end{itemize}
On en déduit que les restrictions à un sous-corps ne préserve pas les égalités entre ensemble décrit par des équations paramétriques et variété associée.
\end{Ex}

\nocite{*}
\bibliographystyle{plain}
\bibliography{biblio}
\end{document}
